## Best suited version: llama3:8b
## Our biggest limitation is the fact that the server has no GPU capable of running a LLM on. However, our capable amount of RAM makes up for that.

## Llama docs:
https://github.com/ollama/ollama/blob/main/docs/linux.md


## Install via docker:
# Pull the docker image for ollama.
docker pull ollama/ollama
# Run the ollama script as name "ollama" and port forward 11434 from the container to the server.
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# Run ollama internally (-it) and pull the correct model version (3:8b).
docker exec -it ollama ollama pull llama3:8b
# Set the restart policy for ollama to be unless stopped by a user.
docker update --restart unless-stopped ollama

# Access lama from command line when it is inside of docker:
docker exec -it ollama ollama run llama3:8b

# Configure for public use. 
# Put files in /opt/ollama-chat
sudo python3 -m venv .venv
sudo chown -R YOUR_USERNAME:YOUR_USERNAME /opt/ollama-chat/
chmod +x /opt/ollama-chat/ollama_chat.sh
sudo ln -s /opt/ollama-chat/ollama_chat.sh /usr/local/bin/ollama-chat
# Now you can run "ollama-chat" from any user account on the server to talk with ollama.

